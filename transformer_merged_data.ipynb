{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Attention, Embedding, Flatten, LayerNormalization, MultiHeadAttention\n"
      ],
      "metadata": {
        "id": "2jASfjLNZ1PK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "aapl_df = pd.read_pickle('/Raw Data/pickle/aapl_prices_labels_news.pkl')\n",
        "amzn_df = pd.read_pickle('/Raw Data/pickle/amzn_prices_labels_news.pkl')\n",
        "msft_df = pd.read_pickle('/Raw Data/pickle/msft_prices_labels_news.pkl')"
      ],
      "metadata": {
        "id": "Zuz8IQz9VYkr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the features and target\n",
        "def data_split(df):\n",
        "  features = df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'neg', 'neu', 'pos', 'compound', 'subjectivity', 'polarity']]\n",
        "  target = df['Adj Close Next']\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "  # Standardize the features\n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  # Reshape the data for the transformer model\n",
        "  X_train = np.expand_dims(X_train, axis=1)\n",
        "  X_test = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n"
      ],
      "metadata": {
        "id": "lvaCz9XZWjBb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Attention Layer\n",
        "class MyAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.Wq = self.add_weight(name=\"Wq\", shape=(input_shape[-1], input_shape[-1]), initializer=\"uniform\", trainable=True)\n",
        "        self.Wk = self.add_weight(name=\"Wk\", shape=(input_shape[-1], input_shape[-1]), initializer=\"uniform\", trainable=True)\n",
        "        self.Wv = self.add_weight(name=\"Wv\", shape=(input_shape[-1], input_shape[-1]), initializer=\"uniform\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = tf.matmul(inputs, self.Wq)\n",
        "        k = tf.matmul(inputs, self.Wk)\n",
        "        v = tf.matmul(inputs, self.Wv)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(tf.shape(k)[-1], tf.float32)))\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "d_GpO1YYZJrp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aapl_X_train, aapl_X_test, aapl_y_train, aapl_y_test = data_split(aapl_df)\n",
        "amzn_X_train, amzn_X_test, amzn_y_train, amzn_y_test = data_split(amzn_df)\n",
        "msft_X_train, msft_X_test, msft_y_train, msft_y_test = data_split(msft_df)"
      ],
      "metadata": {
        "id": "ClrNNvMmHc6m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer model using the custom attention layer\n",
        "def create_transformer_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        MyAttention(),\n",
        "        Dropout(0.2),\n",
        "        LayerNormalization(epsilon=1e-6),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)  # Output layer\n",
        "    ])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "MXOikREJXyN5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "aapl_model = create_transformer_model(aapl_X_train.shape[1:])\n",
        "amzn_model = create_transformer_model(amzn_X_train.shape[1:])\n",
        "msft_model = create_transformer_model(msft_X_train.shape[1:])"
      ],
      "metadata": {
        "id": "nziBIknXXz1Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "models = {'AAPL':[aapl_model, aapl_X_train, aapl_X_test, aapl_y_train, aapl_y_test],\n",
        "          'AMZN': [amzn_model, amzn_X_train, amzn_X_test, amzn_y_train, amzn_y_test],\n",
        "          'MSFT': [msft_model, msft_X_train, msft_X_test, msft_y_train, msft_y_test]}\n",
        "\n",
        "for model in models:\n",
        "  print(f\"{model}\")\n",
        "\n",
        "  transformer = models[model][0]\n",
        "  X_train = models[model][1]\n",
        "  X_test = models[model][2]\n",
        "  y_train = models[model][3]\n",
        "  y_test = models[model][4]\n",
        "\n",
        "  transformer.compile(optimizer='adam', loss='mean_squared_error', metrics = ['mae', 'mape'])\n",
        "\n",
        "  # Train the model\n",
        "  transformer.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)\n",
        "  print(\"\\n\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6zQbjsVY0CV",
        "outputId": "09954708-0c0a-4894-ebb1-ba6309f16acb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAPL\n",
            "Epoch 1/50\n",
            "6/6 [==============================] - 7s 117ms/step - loss: 24419.4766 - mae: 155.3483 - mape: 99.7869 - val_loss: 27562.1074 - val_mae: 165.9826 - val_mape: 99.6389\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 24191.6562 - mae: 154.6019 - mape: 99.2844 - val_loss: 27345.3184 - val_mae: 165.3286 - val_mape: 99.2463\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 23931.0371 - mae: 153.7536 - mape: 98.7253 - val_loss: 27060.4043 - val_mae: 164.4652 - val_mape: 98.7282\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 23574.3848 - mae: 152.6142 - mape: 98.0018 - val_loss: 26674.8438 - val_mae: 163.2898 - val_mape: 98.0230\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 23158.5723 - mae: 151.2379 - mape: 97.0930 - val_loss: 26160.4629 - val_mae: 161.7088 - val_mape: 97.0750\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 22605.9004 - mae: 149.4148 - mape: 95.9055 - val_loss: 25505.3145 - val_mae: 159.6729 - val_mape: 95.8546\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 21895.7754 - mae: 147.0048 - mape: 94.3222 - val_loss: 24607.7109 - val_mae: 156.8413 - val_mape: 94.1582\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 20987.8828 - mae: 143.8905 - mape: 92.2872 - val_loss: 23397.7129 - val_mae: 152.9410 - val_mape: 91.8226\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 19843.6582 - mae: 139.8724 - mape: 89.6657 - val_loss: 21783.8867 - val_mae: 147.5729 - val_mape: 88.6070\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 18306.6621 - mae: 134.2384 - mape: 85.9538 - val_loss: 19659.8320 - val_mae: 140.1850 - val_mape: 84.1784\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 16452.8633 - mae: 127.1283 - mape: 81.3275 - val_loss: 17017.8906 - val_mae: 130.3963 - val_mape: 78.3062\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 14185.3877 - mae: 118.0289 - mape: 75.5871 - val_loss: 13736.0879 - val_mae: 117.0622 - val_mape: 70.3071\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 11593.3291 - mae: 106.3659 - mape: 67.8826 - val_loss: 10232.8037 - val_mae: 100.8899 - val_mape: 60.6049\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 8737.9814 - mae: 91.9366 - mape: 58.6747 - val_loss: 6438.2437 - val_mae: 80.0948 - val_mape: 48.0918\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 6145.9136 - mae: 76.8586 - mape: 49.2799 - val_loss: 3627.0671 - val_mae: 59.4751 - val_mape: 35.7461\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 3795.3347 - mae: 58.8614 - mape: 38.0939 - val_loss: 1463.5593 - val_mae: 36.0504 - val_mape: 21.6954\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 28ms/step - loss: 1975.0117 - mae: 40.1440 - mape: 26.5241 - val_loss: 463.5206 - val_mae: 13.6765 - val_mape: 8.2841\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 1138.5143 - mae: 28.1559 - mape: 18.8612 - val_loss: 333.5659 - val_mae: 13.1238 - val_mape: 7.9801\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 657.4724 - mae: 20.3363 - mape: 13.9938 - val_loss: 545.4045 - val_mae: 22.2376 - val_mape: 13.4143\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 548.7490 - mae: 19.0101 - mape: 13.0572 - val_loss: 545.8575 - val_mae: 22.3118 - val_mape: 13.3881\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 647.4126 - mae: 19.7941 - mape: 14.0644 - val_loss: 415.9431 - val_mae: 19.6915 - val_mape: 11.8280\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 571.7806 - mae: 18.5109 - mape: 13.0517 - val_loss: 285.4165 - val_mae: 16.0788 - val_mape: 9.6592\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 721.7908 - mae: 19.2254 - mape: 13.9791 - val_loss: 184.9167 - val_mae: 13.0015 - val_mape: 7.8253\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 595.9897 - mae: 19.1420 - mape: 13.4230 - val_loss: 126.6028 - val_mae: 10.7453 - val_mape: 6.4791\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 28ms/step - loss: 522.9304 - mae: 18.4191 - mape: 12.8230 - val_loss: 77.8186 - val_mae: 8.1715 - val_mape: 4.9271\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 502.3983 - mae: 17.6234 - mape: 12.3404 - val_loss: 67.6387 - val_mae: 7.3847 - val_mape: 4.4638\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 691.5862 - mae: 19.5441 - mape: 14.1677 - val_loss: 117.7103 - val_mae: 10.3727 - val_mape: 6.2494\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 653.3124 - mae: 19.6524 - mape: 13.8683 - val_loss: 196.2812 - val_mae: 13.6824 - val_mape: 8.2414\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 768.8289 - mae: 20.5478 - mape: 14.4413 - val_loss: 223.2091 - val_mae: 14.6283 - val_mape: 8.8086\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 603.3604 - mae: 18.3703 - mape: 13.1346 - val_loss: 202.8392 - val_mae: 13.9013 - val_mape: 8.3648\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 602.0474 - mae: 18.4563 - mape: 13.0175 - val_loss: 151.5556 - val_mae: 11.6732 - val_mape: 7.0167\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 567.5329 - mae: 18.0442 - mape: 12.7173 - val_loss: 136.4147 - val_mae: 11.0695 - val_mape: 6.6568\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 616.4418 - mae: 18.2648 - mape: 13.1298 - val_loss: 153.5299 - val_mae: 11.8852 - val_mape: 7.1499\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 532.8438 - mae: 17.2856 - mape: 12.2605 - val_loss: 156.6659 - val_mae: 11.8486 - val_mape: 7.1222\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 584.1131 - mae: 17.6351 - mape: 12.6831 - val_loss: 161.0821 - val_mae: 12.1775 - val_mape: 7.3248\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 581.8671 - mae: 18.6983 - mape: 13.2367 - val_loss: 160.0141 - val_mae: 12.2176 - val_mape: 7.3532\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 29ms/step - loss: 505.2105 - mae: 17.3348 - mape: 11.9742 - val_loss: 135.3436 - val_mae: 11.1314 - val_mape: 6.6987\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 538.1839 - mae: 17.0520 - mape: 12.0797 - val_loss: 108.2405 - val_mae: 9.7575 - val_mape: 5.8712\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 598.6167 - mae: 18.8128 - mape: 13.5326 - val_loss: 106.5045 - val_mae: 9.7413 - val_mape: 5.8651\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 591.8421 - mae: 19.0496 - mape: 13.5232 - val_loss: 122.3995 - val_mae: 10.5320 - val_mape: 6.3416\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 571.9684 - mae: 17.7895 - mape: 12.6586 - val_loss: 189.6189 - val_mae: 13.3312 - val_mape: 8.0218\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 559.3676 - mae: 17.9734 - mape: 12.6173 - val_loss: 253.7585 - val_mae: 15.5303 - val_mape: 9.3436\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 498.1201 - mae: 17.6155 - mape: 12.4523 - val_loss: 219.6086 - val_mae: 14.3960 - val_mape: 8.6620\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 51ms/step - loss: 585.5259 - mae: 18.5421 - mape: 13.1789 - val_loss: 166.5447 - val_mae: 12.3976 - val_mape: 7.4583\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 48ms/step - loss: 534.9460 - mae: 17.7259 - mape: 12.7060 - val_loss: 124.9290 - val_mae: 10.4849 - val_mape: 6.3070\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 43ms/step - loss: 557.8792 - mae: 17.2155 - mape: 12.4823 - val_loss: 99.9743 - val_mae: 9.4023 - val_mape: 5.6638\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 486.8401 - mae: 17.2637 - mape: 12.2387 - val_loss: 106.5918 - val_mae: 9.6915 - val_mape: 5.8366\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 559.5017 - mae: 18.2032 - mape: 13.0668 - val_loss: 140.0591 - val_mae: 11.1515 - val_mape: 6.7112\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 551.1855 - mae: 19.2551 - mape: 13.1609 - val_loss: 178.7514 - val_mae: 12.6421 - val_mape: 7.6032\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 575.8710 - mae: 18.3852 - mape: 13.2209 - val_loss: 180.7022 - val_mae: 12.6394 - val_mape: 7.5986\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "AMZN\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 4s 65ms/step - loss: 19279.0840 - mae: 135.8647 - mape: 99.6522 - val_loss: 9157.2178 - val_mae: 95.4486 - val_mape: 99.0401\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 18903.3184 - mae: 134.5158 - mape: 98.6474 - val_loss: 8914.3037 - val_mae: 94.1676 - val_mape: 97.7043\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 18471.1738 - mae: 132.9569 - mape: 97.4909 - val_loss: 8618.3652 - val_mae: 92.5818 - val_mape: 96.0492\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 17843.3145 - mae: 130.6855 - mape: 95.8307 - val_loss: 8198.9746 - val_mae: 90.2874 - val_mape: 93.6553\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 16968.3809 - mae: 127.4514 - mape: 93.4762 - val_loss: 7604.4976 - val_mae: 86.9315 - val_mape: 90.1539\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 15713.3770 - mae: 122.5620 - mape: 89.8401 - val_loss: 6790.1875 - val_mae: 82.1138 - val_mape: 85.1284\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 14011.0020 - mae: 115.7430 - mape: 84.8760 - val_loss: 5753.7231 - val_mae: 75.5385 - val_mape: 78.2690\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 11884.2236 - mae: 106.4941 - mape: 78.1079 - val_loss: 4510.8950 - val_mae: 66.8064 - val_mape: 69.1600\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 9280.9404 - mae: 93.9180 - mape: 68.9698 - val_loss: 3141.1997 - val_mae: 55.6179 - val_mape: 57.4894\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 6439.7896 - mae: 77.5608 - mape: 57.0462 - val_loss: 1790.5881 - val_mae: 41.7478 - val_mape: 43.0230\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 3757.9746 - mae: 58.3768 - mape: 43.0542 - val_loss: 706.0023 - val_mae: 25.6629 - val_mape: 26.2475\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1828.7057 - mae: 38.1767 - mape: 28.4266 - val_loss: 123.4083 - val_mae: 9.6398 - val_mape: 9.6796\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 814.0669 - mae: 22.7477 - mape: 16.9624 - val_loss: 108.1036 - val_mae: 8.5049 - val_mape: 9.2836\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 661.2748 - mae: 20.6690 - mape: 15.4104 - val_loss: 433.2149 - val_mae: 19.6398 - val_mape: 21.0061\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 630.3853 - mae: 19.9613 - mape: 15.2259 - val_loss: 716.2968 - val_mae: 25.8633 - val_mape: 27.4970\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 543.0579 - mae: 18.3258 - mape: 13.9723 - val_loss: 761.9536 - val_mae: 26.7337 - val_mape: 28.4041\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 570.9764 - mae: 18.8766 - mape: 14.3446 - val_loss: 703.4871 - val_mae: 25.6176 - val_mape: 27.2397\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 599.2316 - mae: 19.6738 - mape: 14.8491 - val_loss: 609.3521 - val_mae: 23.7060 - val_mape: 25.2467\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 499.1069 - mae: 17.9365 - mape: 13.3849 - val_loss: 483.5449 - val_mae: 20.8879 - val_mape: 22.3066\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 535.3906 - mae: 18.8374 - mape: 14.2731 - val_loss: 417.0768 - val_mae: 19.2343 - val_mape: 20.5812\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 554.3685 - mae: 19.1698 - mape: 14.3560 - val_loss: 422.3809 - val_mae: 19.3712 - val_mape: 20.7241\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 498.4777 - mae: 17.1573 - mape: 12.7406 - val_loss: 454.4559 - val_mae: 20.1808 - val_mape: 21.5689\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 523.2153 - mae: 18.6233 - mape: 13.8308 - val_loss: 520.0212 - val_mae: 21.7447 - val_mape: 23.2000\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 491.5336 - mae: 18.0686 - mape: 13.7694 - val_loss: 530.4337 - val_mae: 21.9803 - val_mape: 23.4462\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 415.6712 - mae: 15.8739 - mape: 12.0458 - val_loss: 550.0240 - val_mae: 22.4200 - val_mape: 23.9051\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 446.2059 - mae: 16.6304 - mape: 12.4252 - val_loss: 544.9421 - val_mae: 22.3057 - val_mape: 23.7860\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 529.5891 - mae: 18.6578 - mape: 14.1259 - val_loss: 499.4392 - val_mae: 21.2649 - val_mape: 22.6997\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 426.5113 - mae: 16.0258 - mape: 12.2521 - val_loss: 490.8253 - val_mae: 21.0712 - val_mape: 22.4955\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 483.1466 - mae: 17.6918 - mape: 13.4323 - val_loss: 522.1953 - val_mae: 21.8020 - val_mape: 23.2580\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 565.2831 - mae: 18.7810 - mape: 14.3369 - val_loss: 486.4481 - val_mae: 20.9613 - val_mape: 22.3823\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 493.3534 - mae: 18.2067 - mape: 13.8528 - val_loss: 490.2484 - val_mae: 21.0455 - val_mape: 22.4714\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 449.9286 - mae: 16.3849 - mape: 12.2878 - val_loss: 537.6533 - val_mae: 22.1406 - val_mape: 23.6142\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 544.3450 - mae: 18.5865 - mape: 14.0046 - val_loss: 547.3177 - val_mae: 22.3525 - val_mape: 23.8364\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 461.3779 - mae: 17.1275 - mape: 13.1028 - val_loss: 477.5540 - val_mae: 20.7273 - val_mape: 22.1426\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 509.6366 - mae: 18.0949 - mape: 13.5411 - val_loss: 491.0883 - val_mae: 21.0532 - val_mape: 22.4819\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 496.0804 - mae: 18.2805 - mape: 13.8993 - val_loss: 501.4447 - val_mae: 21.3026 - val_mape: 22.7411\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 444.1006 - mae: 16.8965 - mape: 12.9068 - val_loss: 474.8310 - val_mae: 20.6686 - val_mape: 22.0799\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 522.5134 - mae: 18.1627 - mape: 13.5890 - val_loss: 446.3233 - val_mae: 19.9681 - val_mape: 21.3490\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 560.7693 - mae: 19.4837 - mape: 14.6789 - val_loss: 517.6611 - val_mae: 21.6779 - val_mape: 23.1331\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 504.6804 - mae: 17.8344 - mape: 13.5460 - val_loss: 492.0430 - val_mae: 21.0782 - val_mape: 22.5077\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 520.7977 - mae: 18.6492 - mape: 14.0231 - val_loss: 497.2626 - val_mae: 21.2012 - val_mape: 22.6360\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 470.4273 - mae: 17.2731 - mape: 13.0302 - val_loss: 518.6309 - val_mae: 21.6978 - val_mape: 23.1543\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 467.4894 - mae: 17.5559 - mape: 13.1848 - val_loss: 496.3967 - val_mae: 21.1826 - val_mape: 22.6161\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 463.1794 - mae: 16.8028 - mape: 12.4639 - val_loss: 476.9986 - val_mae: 20.7191 - val_mape: 22.1326\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 454.0953 - mae: 17.3623 - mape: 13.1532 - val_loss: 520.2427 - val_mae: 21.7370 - val_mape: 23.1944\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 525.6979 - mae: 18.5512 - mape: 13.7422 - val_loss: 533.3283 - val_mae: 22.0309 - val_mape: 23.5023\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 459.4599 - mae: 16.8222 - mape: 12.9635 - val_loss: 509.8859 - val_mae: 21.4864 - val_mape: 22.9358\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 475.1477 - mae: 17.3916 - mape: 12.9283 - val_loss: 482.5429 - val_mae: 20.8331 - val_mape: 22.2560\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 546.8443 - mae: 19.0704 - mape: 14.2595 - val_loss: 476.8175 - val_mae: 20.6904 - val_mape: 22.1080\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 494.2369 - mae: 18.5040 - mape: 14.0925 - val_loss: 465.6833 - val_mae: 20.4261 - val_mape: 21.8308\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MSFT\n",
            "Epoch 1/50\n",
            "9/9 [==============================] - 2s 32ms/step - loss: 78979.7031 - mae: 278.8949 - mape: 99.7766 - val_loss: 64159.8906 - val_mae: 253.0409 - val_mape: 99.5694\n",
            "Epoch 2/50\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 78087.0312 - mae: 277.3197 - mape: 99.2156 - val_loss: 63568.6016 - val_mae: 251.8735 - val_mape: 99.1105\n",
            "Epoch 3/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 76908.2266 - mae: 275.2564 - mape: 98.4888 - val_loss: 62894.1367 - val_mae: 250.5357 - val_mape: 98.5848\n",
            "Epoch 4/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 75377.8594 - mae: 272.5730 - mape: 97.5590 - val_loss: 62064.1602 - val_mae: 248.8763 - val_mape: 97.9315\n",
            "Epoch 5/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 73283.7109 - mae: 268.8486 - mape: 96.2609 - val_loss: 60990.0273 - val_mae: 246.7055 - val_mape: 97.0742\n",
            "Epoch 6/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 69737.2734 - mae: 262.4346 - mape: 94.0575 - val_loss: 59288.2656 - val_mae: 243.2248 - val_mape: 95.6991\n",
            "Epoch 7/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 65084.9258 - mae: 253.6419 - mape: 90.9937 - val_loss: 56838.1172 - val_mae: 238.1286 - val_mape: 93.6875\n",
            "Epoch 8/50\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 58056.6445 - mae: 239.6840 - mape: 86.2128 - val_loss: 53287.0078 - val_mae: 230.5457 - val_mape: 90.6957\n",
            "Epoch 9/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 48581.2461 - mae: 218.9159 - mape: 78.9914 - val_loss: 48388.6016 - val_mae: 219.6570 - val_mape: 86.4000\n",
            "Epoch 10/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 38413.3789 - mae: 193.3076 - mape: 70.0766 - val_loss: 42372.9883 - val_mae: 205.5185 - val_mape: 80.8281\n",
            "Epoch 11/50\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 27537.6348 - mae: 159.8791 - mape: 58.4814 - val_loss: 34317.4453 - val_mae: 184.6819 - val_mape: 72.7097\n",
            "Epoch 12/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 18835.5117 - mae: 124.0252 - mape: 45.9496 - val_loss: 26399.1699 - val_mae: 160.1479 - val_mape: 63.1382\n",
            "Epoch 13/50\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 12344.9785 - mae: 90.7558 - mape: 34.3042 - val_loss: 18721.5508 - val_mae: 134.3398 - val_mape: 52.9512\n",
            "Epoch 14/50\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 8917.9404 - mae: 74.3318 - mape: 28.2238 - val_loss: 11907.7676 - val_mae: 108.0490 - val_mape: 42.5010\n",
            "Epoch 15/50\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 6601.2896 - mae: 65.3717 - mape: 24.7401 - val_loss: 6459.5513 - val_mae: 79.3904 - val_mape: 31.1733\n",
            "Epoch 16/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 4690.3853 - mae: 58.1832 - mape: 21.4257 - val_loss: 2813.4407 - val_mae: 51.9481 - val_mape: 20.3244\n",
            "Epoch 17/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 3015.4480 - mae: 46.5317 - mape: 17.3093 - val_loss: 873.2810 - val_mae: 27.5458 - val_mape: 10.7053\n",
            "Epoch 18/50\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 2320.8254 - mae: 40.0942 - mape: 15.0288 - val_loss: 148.4993 - val_mae: 9.6699 - val_mape: 3.7358\n",
            "Epoch 19/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1747.5153 - mae: 33.0211 - mape: 12.5613 - val_loss: 189.4472 - val_mae: 11.2522 - val_mape: 4.5563\n",
            "Epoch 20/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1833.2771 - mae: 33.8047 - mape: 12.7248 - val_loss: 444.6222 - val_mae: 17.2866 - val_mape: 6.9325\n",
            "Epoch 21/50\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1596.3781 - mae: 31.5977 - mape: 11.9338 - val_loss: 1741.9862 - val_mae: 24.1101 - val_mape: 9.4651\n",
            "Epoch 22/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 2283.8401 - mae: 36.9478 - mape: 13.9398 - val_loss: 1859.3711 - val_mae: 25.6338 - val_mape: 10.0626\n",
            "Epoch 23/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1928.3147 - mae: 33.9269 - mape: 13.0111 - val_loss: 1057.8059 - val_mae: 23.5235 - val_mape: 9.3174\n",
            "Epoch 24/50\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1945.1237 - mae: 34.6263 - mape: 13.2039 - val_loss: 496.0767 - val_mae: 19.4160 - val_mape: 7.8102\n",
            "Epoch 25/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1869.6973 - mae: 33.3519 - mape: 12.6180 - val_loss: 445.7635 - val_mae: 17.7774 - val_mape: 7.1432\n",
            "Epoch 26/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1683.9535 - mae: 32.1189 - mape: 12.1107 - val_loss: 635.9054 - val_mae: 19.7200 - val_mape: 7.8676\n",
            "Epoch 27/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1793.0807 - mae: 33.5827 - mape: 12.5575 - val_loss: 1074.1034 - val_mae: 21.5753 - val_mape: 8.5293\n",
            "Epoch 28/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1913.3776 - mae: 35.1726 - mape: 13.4076 - val_loss: 1227.3259 - val_mae: 22.3527 - val_mape: 8.8193\n",
            "Epoch 29/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1697.4352 - mae: 33.0435 - mape: 12.5683 - val_loss: 1199.4313 - val_mae: 20.9493 - val_mape: 8.2559\n",
            "Epoch 30/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1823.7588 - mae: 33.0768 - mape: 12.5494 - val_loss: 1123.7181 - val_mae: 21.3814 - val_mape: 8.4424\n",
            "Epoch 31/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1945.2469 - mae: 34.6812 - mape: 13.1023 - val_loss: 1492.7107 - val_mae: 25.2549 - val_mape: 9.9510\n",
            "Epoch 32/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1652.5219 - mae: 32.6594 - mape: 12.2789 - val_loss: 1685.5291 - val_mae: 25.3286 - val_mape: 9.9587\n",
            "Epoch 33/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 2053.2371 - mae: 35.4514 - mape: 13.4222 - val_loss: 1716.6193 - val_mae: 24.3315 - val_mape: 9.5562\n",
            "Epoch 34/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1759.5366 - mae: 32.5699 - mape: 12.4447 - val_loss: 1716.3235 - val_mae: 24.8842 - val_mape: 9.7779\n",
            "Epoch 35/50\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1740.4023 - mae: 32.0132 - mape: 12.3538 - val_loss: 1678.9468 - val_mae: 24.0347 - val_mape: 9.4404\n",
            "Epoch 36/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 2007.5284 - mae: 34.9535 - mape: 13.1583 - val_loss: 1539.0406 - val_mae: 21.2327 - val_mape: 8.3213\n",
            "Epoch 37/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1665.7396 - mae: 32.6924 - mape: 12.4245 - val_loss: 1576.5712 - val_mae: 21.5270 - val_mape: 8.4364\n",
            "Epoch 38/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1652.2329 - mae: 32.0865 - mape: 12.1468 - val_loss: 1632.6271 - val_mae: 20.4500 - val_mape: 7.9922\n",
            "Epoch 39/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 2065.3108 - mae: 35.2864 - mape: 13.4737 - val_loss: 1731.5981 - val_mae: 22.5234 - val_mape: 8.8240\n",
            "Epoch 40/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1738.8716 - mae: 33.0804 - mape: 12.3277 - val_loss: 1800.6985 - val_mae: 24.5849 - val_mape: 9.6476\n",
            "Epoch 41/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1683.8645 - mae: 31.3472 - mape: 11.9747 - val_loss: 1973.5707 - val_mae: 25.7361 - val_mape: 10.0893\n",
            "Epoch 42/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1986.9065 - mae: 35.6556 - mape: 13.3431 - val_loss: 2077.4856 - val_mae: 25.0215 - val_mape: 9.7959\n",
            "Epoch 43/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1921.9043 - mae: 33.9609 - mape: 12.5285 - val_loss: 1601.8816 - val_mae: 25.1439 - val_mape: 9.8948\n",
            "Epoch 44/50\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 1865.7184 - mae: 34.4288 - mape: 13.1481 - val_loss: 1415.9000 - val_mae: 22.8203 - val_mape: 8.9826\n",
            "Epoch 45/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 2064.2285 - mae: 36.0587 - mape: 13.6891 - val_loss: 1509.4321 - val_mae: 23.7321 - val_mape: 9.3375\n",
            "Epoch 46/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 2013.5630 - mae: 35.3244 - mape: 13.4009 - val_loss: 1163.0054 - val_mae: 25.3056 - val_mape: 10.0102\n",
            "Epoch 47/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1747.0795 - mae: 31.8701 - mape: 12.0155 - val_loss: 1462.4780 - val_mae: 27.1816 - val_mape: 10.7222\n",
            "Epoch 48/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1873.4890 - mae: 34.6134 - mape: 13.1354 - val_loss: 1897.2776 - val_mae: 28.1950 - val_mape: 11.0775\n",
            "Epoch 49/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1887.5151 - mae: 33.0490 - mape: 12.7673 - val_loss: 1659.3210 - val_mae: 25.9323 - val_mape: 10.2010\n",
            "Epoch 50/50\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 1959.8020 - mae: 33.9845 - mape: 13.1232 - val_loss: 1572.4961 - val_mae: 23.5257 - val_mape: 9.2454\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "\n",
        "  transformer = models[model][0]\n",
        "  X_train = models[model][1]\n",
        "  X_test = models[model][2]\n",
        "  y_train = models[model][3]\n",
        "  y_test = models[model][4]\n",
        "\n",
        "  # Evaluate the model on the test set\n",
        "  y_pred = transformer.predict(X_test)\n",
        "\n",
        "\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  evals = transformer.evaluate(X_test, y_test)\n",
        "\n",
        "  print(f\"{model}\")\n",
        "\n",
        "  print(f'Mean Squared Error on Test Set: {mse}')\n",
        "  print(f'Root Mean Squared Error on Test Set: {mse**(1/2)}')\n",
        "  print(f'Mean Absolute Error on Test Set: {evals[1]}')\n",
        "  print(f'Mean Absolute Percentage Error on Test Set: {evals[2]}\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GDWWpVNK9M_",
        "outputId": "9b0c915a-2e4f-4f68-f2e5-a538f35a92d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 8ms/step\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 74.2661 - mae: 7.6158 - mape: 4.1266\n",
            "AAPL\n",
            "Mean Squared Error on Test Set: 74.26611051102238\n",
            "Root Mean Squared Error on Test Set: 8.617778745768678\n",
            "Mean Absolute Error on Test Set: 7.615825653076172\n",
            "Mean Absolute Percentage Error on Test Set: 4.126607894897461\n",
            "\n",
            "\n",
            "3/3 [==============================] - 0s 5ms/step\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 648.6970 - mae: 21.7743 - mape: 16.9434\n",
            "AMZN\n",
            "Mean Squared Error on Test Set: 648.6970034859165\n",
            "Root Mean Squared Error on Test Set: 25.469530884684872\n",
            "Mean Absolute Error on Test Set: 21.774253845214844\n",
            "Mean Absolute Percentage Error on Test Set: 16.94340705871582\n",
            "\n",
            "\n",
            "3/3 [==============================] - 0s 6ms/step\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 537.4771 - mae: 16.4564 - mape: 5.3388\n",
            "MSFT\n",
            "Mean Squared Error on Test Set: 537.4772570860044\n",
            "Root Mean Squared Error on Test Set: 23.18355574725336\n",
            "Mean Absolute Error on Test Set: 16.456403732299805\n",
            "Mean Absolute Percentage Error on Test Set: 5.338837146759033\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}